{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-25T09:03:45.955813278Z",
     "start_time": "2025-12-25T09:03:44.343308942Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T09:04:07.843133867Z",
     "start_time": "2025-12-25T09:04:07.814314048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def measure_tops(matrix_size, num_iterations):\n",
    "    # Create two random matrices\n",
    "    a = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    b = torch.randn(matrix_size, matrix_size).cuda()\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        torch.matmul(a, b)\n",
    "\n",
    "    # Measure the elapsed time for multiple iterations\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        torch.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate operations and TOPS\n",
    "    total_operations = 2 * (matrix_size ** 3) * num_iterations  # 2 operations per element in matrix multiplication\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "    tops = total_operations / elapsed_time / 1e12  # convert to TOPS\n",
    "\n",
    "    return tops"
   ],
   "id": "adb767521117f25",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T09:04:12.517357447Z",
     "start_time": "2025-12-25T09:04:12.322859508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matrix_size = 512  # Adjust as needed\n",
    "num_iterations = 100  # Number of iterations for benchmarking\n",
    "\n",
    "tops = measure_tops(matrix_size, num_iterations)\n",
    "print(f\"TOPS: {tops:.2f} TOPS for matrix size {matrix_size} over {num_iterations} iterations.\")\n"
   ],
   "id": "eec35f9e9a2048d2",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m matrix_size = \u001B[32m512\u001B[39m  \u001B[38;5;66;03m# Adjust as needed\u001B[39;00m\n\u001B[32m      2\u001B[39m num_iterations = \u001B[32m100\u001B[39m  \u001B[38;5;66;03m# Number of iterations for benchmarking\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m tops = \u001B[43mmeasure_tops\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_iterations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTOPS: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtops\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m TOPS for matrix size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmatrix_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m over \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_iterations\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m iterations.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 6\u001B[39m, in \u001B[36mmeasure_tops\u001B[39m\u001B[34m(matrix_size, num_iterations)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmeasure_tops\u001B[39m(matrix_size, num_iterations):\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# Create two random matrices\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     a = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatrix_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     b = torch.randn(matrix_size, matrix_size).cuda()\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# Warm-up\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/play-TRexRunner/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:410\u001B[39m, in \u001B[36m_lazy_init\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[32m    406\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m     )\n\u001B[32m    408\u001B[39m \u001B[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001B[39;00m\n\u001B[32m    409\u001B[39m \u001B[38;5;66;03m# are found or any other error occurs\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m410\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[32m    412\u001B[39m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[32m    413\u001B[39m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[32m    414\u001B[39m _tls.is_initializing = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T09:05:55.808922030Z",
     "start_time": "2025-12-25T09:05:55.164403386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ],
   "id": "e341c4d092afef3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezio4df/projects/play-TRexRunner/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
